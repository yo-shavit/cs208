{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow privacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/bd/3x3rvhhx6t9d8zwq94l4jh6c0000gn/T/tmpq4y78060\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/bd/3x3rvhhx6t9d8zwq94l4jh6c0000gn/T/tmpq4y78060', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c34e253c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /Users/tercer/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/tercer/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /Users/tercer/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /Users/tercer/Scratch/classes/cs208/junk/privacy/privacy/dp_query/gaussian_query.py:49: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/tercer/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tercer/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:Issue encountered when serializing critical_section_executions.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_ExecutionSignature' object has no attribute 'name'\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /Users/tercer/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:Issue encountered when serializing critical_section_executions.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_ExecutionSignature' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/bd/3x3rvhhx6t9d8zwq94l4jh6c0000gn/T/tmpq4y78060/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing critical_section_executions.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_ExecutionSignature' object has no attribute 'name'\n",
      "INFO:tensorflow:loss = 2.3145194, step = 1\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2018, The TensorFlow Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Training a CNN on MNIST with differentially private SGD optimizer.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from privacy.analysis import privacy_ledger\n",
    "from privacy.analysis.rdp_accountant import compute_rdp_from_ledger\n",
    "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from privacy.optimizers import dp_optimizer\n",
    "\n",
    "# Compatibility with tf 1 and 2 APIs\n",
    "try:\n",
    "  GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n",
    "except:  # pylint: disable=bare-except\n",
    "  GradientDescentOptimizer = tf.optimizers.SGD  # pylint: disable=invalid-name\n",
    "\n",
    "tf.flags.DEFINE_boolean('dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
    "                        'train with vanilla SGD.')\n",
    "tf.flags.DEFINE_float('learning_rate', .15, 'Learning rate for training')\n",
    "tf.flags.DEFINE_float('noise_multiplier', 1.1,\n",
    "                      'Ratio of the standard deviation to the clipping norm')\n",
    "tf.flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
    "tf.flags.DEFINE_integer('batch_size', 256, 'Batch size')\n",
    "tf.flags.DEFINE_integer('epochs', 60, 'Number of epochs')\n",
    "tf.flags.DEFINE_integer('microbatches', 256, 'Number of microbatches '\n",
    "                        '(must evenly divide batch_size)')\n",
    "tf.flags.DEFINE_string('model_dir', None, 'Model directory')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "class EpsilonPrintingTrainingHook(tf.estimator.SessionRunHook):\n",
    "  \"\"\"Training hook to print current value of epsilon after an epoch.\"\"\"\n",
    "\n",
    "  def __init__(self, ledger):\n",
    "    \"\"\"Initalizes the EpsilonPrintingTrainingHook.\n",
    "\n",
    "    Args:\n",
    "      ledger: The privacy ledger.\n",
    "    \"\"\"\n",
    "    self._samples, self._queries = ledger.get_unformatted_ledger()\n",
    "\n",
    "  def end(self, session):\n",
    "    orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
    "    samples = session.run(self._samples)\n",
    "    queries = session.run(self._queries)\n",
    "    formatted_ledger = privacy_ledger.format_ledger(samples, queries)\n",
    "    rdp = compute_rdp_from_ledger(formatted_ledger, orders)\n",
    "    eps = get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
    "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for a CNN.\"\"\"\n",
    "\n",
    "  # Define CNN architecture using tf.keras.layers.\n",
    "  input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\n",
    "  y = tf.keras.layers.Conv2D(16, 8,\n",
    "                             strides=2,\n",
    "                             padding='same',\n",
    "                             activation='relu').apply(input_layer)\n",
    "  y = tf.keras.layers.MaxPool2D(2, 1).apply(y)\n",
    "  y = tf.keras.layers.Conv2D(32, 4,\n",
    "                             strides=2,\n",
    "                             padding='valid',\n",
    "                             activation='relu').apply(y)\n",
    "  y = tf.keras.layers.MaxPool2D(2, 1).apply(y)\n",
    "  y = tf.keras.layers.Flatten().apply(y)\n",
    "  y = tf.keras.layers.Dense(32, activation='relu').apply(y)\n",
    "  logits = tf.keras.layers.Dense(10).apply(y)\n",
    "\n",
    "  # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
    "  vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=labels, logits=logits)\n",
    "  # Define mean of loss across minibatch (for reporting through tf.Estimator).\n",
    "  scalar_loss = tf.reduce_mean(vector_loss)\n",
    "\n",
    "  # Configure the training op (for TRAIN mode).\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "    if FLAGS.dpsgd:\n",
    "      ledger = privacy_ledger.PrivacyLedger(\n",
    "          population_size=60000,\n",
    "          selection_probability=(FLAGS.batch_size / 60000),\n",
    "          max_samples=1e6,\n",
    "          max_queries=1e6)\n",
    "\n",
    "      # Use DP version of GradientDescentOptimizer. Other optimizers are\n",
    "      # available in dp_optimizer. Most optimizers inheriting from\n",
    "      # tf.train.Optimizer should be wrappable in differentially private\n",
    "      # counterparts by calling dp_optimizer.optimizer_from_args().\n",
    "      optimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(\n",
    "          l2_norm_clip=FLAGS.l2_norm_clip,\n",
    "          noise_multiplier=FLAGS.noise_multiplier,\n",
    "          num_microbatches=FLAGS.microbatches,\n",
    "          ledger=ledger,\n",
    "          learning_rate=FLAGS.learning_rate)\n",
    "      training_hooks = [\n",
    "          EpsilonPrintingTrainingHook(ledger)\n",
    "      ]\n",
    "      opt_loss = vector_loss\n",
    "    else:\n",
    "      optimizer = GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "      training_hooks = []\n",
    "      opt_loss = scalar_loss\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss=opt_loss, global_step=global_step)\n",
    "    # In the following, we pass the mean of the loss (scalar_loss) rather than\n",
    "    # the vector_loss because tf.estimator requires a scalar loss. This is only\n",
    "    # used for evaluation and debugging by tf.estimator. The actual loss being\n",
    "    # minimized is opt_loss defined above and passed to optimizer.minimize().\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=scalar_loss,\n",
    "                                      train_op=train_op,\n",
    "                                      training_hooks=training_hooks)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode).\n",
    "  elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "    eval_metric_ops = {\n",
    "        'accuracy':\n",
    "            tf.metrics.accuracy(\n",
    "                labels=labels,\n",
    "                predictions=tf.argmax(input=logits, axis=1))\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=scalar_loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "  assert train_labels.ndim == 1\n",
    "  assert test_labels.ndim == 1\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  if FLAGS.batch_size % FLAGS.microbatches != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "  # Load training and test data.\n",
    "  train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "  # Instantiate the tf.Estimator.\n",
    "  mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                            model_dir=FLAGS.model_dir)\n",
    "\n",
    "  # Create tf.Estimator input functions for the training and test data.\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={'x': train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=FLAGS.batch_size,\n",
    "      num_epochs=FLAGS.epochs,\n",
    "      shuffle=True)\n",
    "  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={'x': test_data},\n",
    "      y=test_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "  # Training loop.\n",
    "  steps_per_epoch = 60000 // FLAGS.batch_size\n",
    "  for epoch in range(1, FLAGS.epochs + 1):\n",
    "    # Train the model for one epoch.\n",
    "    mnist_classifier.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    test_accuracy = eval_results['accuracy']\n",
    "    print('Test accuracy after %d epochs is: %.3f' % (epoch, test_accuracy))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
