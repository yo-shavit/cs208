{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4b Submission\n",
    "## Yonadav Shavit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from scipy.special import factorial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Let's define some latex commands)\n",
    " \n",
    "$\\newcommand{\\floor}[1]{\\left \\lfloor #1 \\right \\rfloor}$\n",
    "$\\newcommand{\\clamp}[1]{\\left [ #1 \\right ]}$\n",
    "$\\newcommand{\\lap}[1]{Lap\\left ( #1 \\right )}$\n",
    "$\\newcommand{\\lapcdf}[2]{LapCDF\\left ( #1, [-\\inf, #2] \\right )}$\n",
    "$\\newcommand{\\lappdf}[2]{LapPDF\\left ( #1, |_{#2} \\right )}$\n",
    "$\\newcommand{\\lapcdffull}[2]{\\frac{1}{2} + \\frac{1}{2}sgn\\left({#2}\\right)\\left ( 1 - \\exp{\\left ( - \\frac{|#2|}{#1} \\right ) }\\right )}$\n",
    "$\\newcommand{\\lappdffull}[2]{\\frac{1}{2 #1} \\exp{\\left ( - \\frac{#2}{#1}\\right ) }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "### (a)\n",
    "#### i.\n",
    "$GS_f = \\infty$ because the sensitivity is unbounded and so a single point can arbitrarily change the mean.\n",
    "#### ii.\n",
    "$\\min _{x \\in \\mathcal{S}} \\operatorname{LS}_{f}(x) = \\infty$ for the same reasons as above - regardless of any of the other points (i.e. $x$ s.t. we get minimum local sensitivity), a single additional point can arbitrarily change the resulting mean. (For example, to increase the mean by $k$, the new point should be $\\mu_{old} + nk$.)\n",
    "#### iii.\n",
    "$\\mathrm{RS}_{f}^{\\mathcal{H}} = \\frac{b-a}{n}$ because a single point can change only by $b-a$ while remaining in $H$, and so the average can only change by that divided by $n$.\n",
    "#### iv.\n",
    "We can construct an explicit Lipschitz extension by simply clipping each point to $[a, b]$ and then taking the mean and adding Laplace noise with sensitivity $\\frac{b-a}{n\\epsilon}$. This will be increasingly biased as the original data is farther from $[a, b]$, but the sensitivity will remain the same, and on $H$ this agrees with $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "#### i.\n",
    "$GS_f = \\infty$ because, for any large $N$, we can construct a dataset with $\\frac{n}{2}$ data points at $0$, and $\\frac{n}{2}$ datapoints at $N$, and then shifting a single point from $0$ to $N$ shifts the median by $N$, for any arbitrary $N$.\n",
    "#### ii.\n",
    "$\\min _{x \\in \\mathcal{S}} \\operatorname{LS}_{f}(x) = 0$ because if we create a dataset with $3$ points at $0$, no matter how much we change any single point, the other two will still be $0$ and so the median will be unchanged.\n",
    "#### iii.\n",
    "$\\mathrm{RS}_{f}^{\\mathcal{H}} = b-a$ because every point must fall in $[a, b]$, and so the largest possible change in median would be from $a$ to $b$. (This example is possible if half the points minus 1 are at $a$, and the other half are at $b$, and then we move one from $b$ to $a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "#### i.\n",
    "$GS_f = n-1$ because if every node was previously disconnected (meaning the value was $n-1$) we can connect a new node to every old node, meaning that now $0$ nodes are disconnected.\n",
    "#### ii.\n",
    "$\\min _{x \\in \\mathcal{S}} \\operatorname{LS}_{f}(x) = 1$ because if every node in the graph is already connected, the addition of a new node cannot disconnect any existing nodes. The most it can do is add a node with no edges. (Note that our definition of node sensitivity in this case is a little weird, because if we're talking about deleting nodes rather than adding, the minimum local sensitivity is actually $0$, if all edges exist between all nodes.)\n",
    "#### iii.\n",
    "$\\mathrm{RS}_{f}^{\\mathcal{H}} = d$ because the greatest change from adding a single node would be that all existing nodes are unconnected, and then a single new node is added that connects to $d$ other nodes. \n",
    "(It can't connect to more than that because it must be in $H$). \n",
    "Thus the query can only decrease by $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the R code, reproduced. I'm porting it into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the likelihood function for a Logit\n",
    "# calcllik<-function(b,data){           \n",
    "#   y<-data[,1]\n",
    "#   x<-data[,2]\n",
    "\n",
    "#   pi<- 1/(1+exp(-b[1] - b[2]*x))        # Here is the systematic component\n",
    "\n",
    "#   llik<-y * log(pi) + (1-y) * log(1-pi) # Here is the stocastic component\n",
    "#   return(-llik)\n",
    "# }\n",
    "\n",
    "def likelihood(w, b, x, y):\n",
    "    pi = 1/(1 + np.exp(-b - w*x.reshape(-1)))\n",
    "    llik = y*np.log(pi + 1e-9) + (1-y)*np.log(1e-9 + 1-pi)\n",
    "    print(f\"likelihood var: {llik.std()}\")\n",
    "    return - llik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Differentially private mean release\n",
    "\n",
    "# gaussianReleaseNoise <- function(size=1, sensitivity, epsilon, delta){\n",
    "# \tscale <- sensitivity *log(1.25/delta)/ epsilon\n",
    "# \tnoise <- rnorm(n=size, mean=0, sd=scale)\n",
    "# \treturn(noise)\n",
    "# }\n",
    "\n",
    "def gaussianReleaseNoise(sensitivity, epsilon, delta, size=1):\n",
    "    scale = sensitivity*np.log(1.25/delta)/epsilon\n",
    "    noise = np.random.normal(scale=scale, size=size)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FutureWarning",
     "evalue": "Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFutureWarning\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-92c9273ec8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"slope = {lr.coef_[0]}, bias = {lr.intercept_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1278\u001b[0m                              \"positive; got (tol=%r)\" % self.tol)\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_check_solver\u001b[0;34m(solver, penalty, dual)\u001b[0m\n\u001b[1;32m    431\u001b[0m         warnings.warn(\"Default solver will be changed to 'lbfgs' in 0.22. \"\n\u001b[1;32m    432\u001b[0m                       \u001b[0;34m\"Specify a solver to silence this warning.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                       FutureWarning)\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mall_solvers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saga'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFutureWarning\u001b[0m: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/yonadavshavit/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m(433)\u001b[0;36m_check_solver\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    431 \u001b[0;31m        warnings.warn(\"Default solver will be changed to 'lbfgs' in 0.22. \"\n",
      "\u001b[0m\u001b[0;32m    432 \u001b[0;31m                      \u001b[0;34m\"Specify a solver to silence this warning.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 433 \u001b[0;31m                      FutureWarning)\n",
      "\u001b[0m\u001b[0;32m    434 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    435 \u001b[0;31m    \u001b[0mall_solvers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saga'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "#### Run with actual data\n",
    "\n",
    "# library(\"foreign\")\n",
    "# PUMSdata <- read.csv(file=\"../../data/MaPUMS5full.csv\")\n",
    "\n",
    "# mydata<-PUMSdata[c(\"married\",\"educ\")]\n",
    "\n",
    "# output <- glm(married ~ educ, family=\"binomial\", data=mydata)\n",
    "\n",
    "# print(summary(output))\n",
    "\n",
    "PUMSdata = pd.read_csv(\"../data/MaPUMS5full.csv\")\n",
    "X = (PUMSdata['educ'].values).reshape(-1, 1)\n",
    "Y = PUMSdata['married'].values\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=10000).fit(X, Y)\n",
    "\n",
    "print(f\"slope = {lr.coef_[0]}, bias = {lr.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Show the estimated model\n",
    "\n",
    "# xseq <- seq(from=-40, to=60, length=100)\n",
    "# f <- 1/(1 + exp(-output$coef[1] -output$coef[2]*xseq))\n",
    "\n",
    "# par(mfcol=c(2,1))\n",
    "\n",
    "# plot(xseq, f, type=\"l\", lwd=1.5, col=\"red\", ylim=c(0,1), ylab=\"E(y|x,theta)\", xlab=\"education\", main=\"Probability Married by Education\")\n",
    "# abline(v=1, col=\"blue\", lty=3)\n",
    "# abline(v=16, col=\"blue\", lty=3)\n",
    "\n",
    "# plot(xseq, f, type=\"l\", lwd=1.5, col=\"red\", ylim=c(0,1), ylab=\"E(y|x,theta)\", xlab=\"education\", xlim=c(-5,20))\n",
    "# for(i in 1:16){\n",
    "# \tflag<-mydata$educ==i\n",
    "# \tpoints(x=i, y=mean(mydata[flag,\"married\"]))\n",
    "# }\n",
    "# dev.copy2pdf(file=\"./figs/married_educ.pdf\")\n",
    "\n",
    "\n",
    "# #### Show the LogLikelihood surface\n",
    "\n",
    "# sample.data <- mydata[sample(1:nrow(mydata),10000), ]\n",
    "# b1.seq <- seq(from=-3, to=2, length=25)\n",
    "# b2.seq <- seq(from=-.2, to=.3, length=25)\n",
    "# llsurface <- matrix(NA, nrow=length(b1.seq), ncol=length(b2.seq))\n",
    "\n",
    "# for(i in 1:length(b1.seq)){\n",
    "# \tfor(j in 1:length(b2.seq)){\n",
    "# \t\tllsurface[i,j] <- sum(-1* calcllik(b=c(b1.seq[i], b2.seq[j]), data=sample.data) )\n",
    "# \t}\n",
    "# }\n",
    "\n",
    "# filled.contour(x=b1.seq, y=b2.seq, z=llsurface, color = terrain.colors,  xlab=\"constant parameter\", ylab=\"education parameter\")\n",
    "\n",
    "# dev.copy2pdf(file=\"./figs/logitLLike.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcgradient <- function(B, C, theta, fun){\n",
    "# \tdx <- 0.0001\n",
    "\n",
    "# \tout1 <-\teval(fun(b=theta, data=B))\n",
    "# \tout2 <- eval(fun(b=theta + c(0,dx), data=B))\n",
    "# \tout3 <- eval(fun(b=theta + c(dx,0), data=B))\n",
    "\n",
    "# \tDel.1 <- 1\n",
    "# \t# Del.1 <- clip(Del.1, lower=0, upper=1)  # Fix this\n",
    "# \tmean.Del.1 <- mean(Del.1)\n",
    "\n",
    "# \tDel.2 <- 1\n",
    "# \t# Del.2 <- clip(Del.2, lower=0, upper=1)  # Fix this\n",
    "# \tmean.Del.2 <- mean(Del.2)\n",
    "\n",
    "# \treturn(c(mean.Del.1,mean.Del.2))\n",
    "# }\n",
    "\n",
    "def calc_clipped_gradient(X, Y, C, w, b, fun=likelihood):\n",
    "    dx = 1e-4\n",
    "    out1 = fun(w     ,      b, X, Y)\n",
    "    out2 = fun(w     , b + dx, X, Y)\n",
    "    out3 = fun(w + dx, b     , X, Y)\n",
    "    dw = (out3 - out1)/dx\n",
    "    db = (out2 - out1)/dx\n",
    "    scaling_param = np.minimum(1, C/np.sqrt(dw**2 + db**2))\n",
    "    return dw*scaling_param, db*scaling_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N <- nrow(mydata)\n",
    "# L <- round(sqrt(nrow(mydata)))\n",
    "\n",
    "# steps <- 10   \t  # Fix this\n",
    "\n",
    "# ## Shuffle the data\n",
    "# index <- sample(1:nrow(mydata))\n",
    "# mydata <- mydata[index,]\n",
    "# epsilon <-1\n",
    "\n",
    "# theta <- c(0,0)   # Starting parameters\n",
    "# C <- 10\t\t\t  # Interval to clip over\n",
    "# nu <- c(1,0.01)   # Learning speeds\n",
    "\n",
    "\n",
    "# history <- matrix(NA, nrow=steps+1, ncol=2)\n",
    "# history[1,] <- theta\n",
    "\n",
    "# for(i in 1:steps){\n",
    "# \tstartB <- ((i-1)*L+1)\n",
    "# \tif(i<L){\n",
    "# \t\tstopB <- i*L\n",
    "# \t}else{\n",
    "# \t\tstopB <- nrow(mydata)\n",
    "# \t}\n",
    "\n",
    "# \tindex<-sample(1:nrow(mydata),L)\n",
    "# \tB <- mydata[startB:stopB, ]\n",
    "# \tDel <- calcgradient(B, C, theta, fun=calcllik)\n",
    "# \tcat(\"Del:  \",Del,\"\\n\")\n",
    "# \ttheta <- theta   \t\t\t\t# Fix this\n",
    "# \tcat(\"Theta:\",theta, \"\\n\")\n",
    "\n",
    "# \thistory[i+1,] <- theta\n",
    "\n",
    "# }\n",
    "def learn_private_regression(epsilon, X_, Y_, delta=1e-6):\n",
    "    n = Y_.shape[0]\n",
    "    batchsize = int(np.sqrt(n))\n",
    "    steps = int(np.sqrt(n))\n",
    "\n",
    "    idxs = np.random.permutation(n)\n",
    "    X = X_[idxs, :]\n",
    "    Y = Y_[idxs]\n",
    "\n",
    "    w = 0; b = 0         # starting parameters\n",
    "    C = 10               # clipping parameter\n",
    "    nu_w, nu_b = .00001, .001  # learning rates\n",
    "\n",
    "    w_history = [w]\n",
    "    b_history = [b]\n",
    "    \n",
    "    for i in tqdm_notebook(range(steps)):\n",
    "        print(f\"b:{b}\")\n",
    "        batch_start = i*batchsize\n",
    "        batch_end = min((i + 1)*batchsize, n)\n",
    "        Xbatch = X[batch_start:batch_end, :]\n",
    "        Ybatch = Y[batch_start:batch_end]\n",
    "        dw_clipped, db_clipped = calc_clipped_gradient(Xbatch, Ybatch, C, w, b, fun=likelihood)\n",
    "        dw_private = dw_clipped + gaussianReleaseNoise(\n",
    "            sensitivity=2*C, # one sample can shift from -C to C, so double sensitivity\n",
    "            epsilon=epsilon/(batchsize*steps*2), # two parameters released, so halve the epsilon for each\n",
    "            delta=delta,\n",
    "            size=batchsize,\n",
    "        )\n",
    "        db_private = db_clipped  + gaussianReleaseNoise(\n",
    "            sensitivity=2*C/batchsize, # one sample can shift from -C to C, so double sensitivity\n",
    "            epsilon=(epsilon/(2*steps)), # two parameters released, so halve the epsilon for each\n",
    "            delta=delta,\n",
    "            size=batchsize,\n",
    "        )\n",
    "        dw_private = dw_private.mean()\n",
    "        db_private = db_private.mean()\n",
    "        # update via calculated gradients\n",
    "        w += nu_w*dw_private\n",
    "        b += nu_b*db_private\n",
    "        w_history.append(w)\n",
    "        b_history.append(b)\n",
    "        if True:\n",
    "            print(f\"Likelihood: {likelihood(w, b, X, Y).mean()}\")\n",
    "    return w_history, b_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cc21315ec54a469413bfcef2fbb49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:0\n",
      "likelihood var: 2.7103998191910124e-16\n",
      "likelihood var: 4.853864430087968e-05\n",
      "likelihood var: 0.0005318635355073509\n",
      "likelihood var: 10.000500815710028\n",
      "Likelihood: 8.23940923357495\n",
      "b:0.04231599160889713\n",
      "likelihood var: 10.024761260755643\n",
      "likelihood var: 10.02476334213856\n",
      "likelihood var: 10.024772218600631\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeWarning",
     "evalue": "divide by zero encountered in true_divide",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-23403d7c5915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_private_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-000ee202c94f>\u001b[0m in \u001b[0;36mlearn_private_regression\u001b[0;34m(epsilon, X_, Y_, delta)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mXbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mYbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mdw_clipped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_clipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_clipped_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         dw_private = dw_clipped + gaussianReleaseNoise(\n\u001b[1;32m     61\u001b[0m             \u001b[0msensitivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# one sample can shift from -C to C, so double sensitivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-be4d921c9216>\u001b[0m in \u001b[0;36mcalc_clipped_gradient\u001b[0;34m(X, Y, C, w, b, fun)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mscaling_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscaling_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscaling_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeWarning\u001b[0m: divide by zero encountered in true_divide"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-17-be4d921c9216>\u001b[0m(26)\u001b[0;36mcalc_clipped_gradient\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     23 \u001b[0;31m    \u001b[0mout3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m    \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m    \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m    \u001b[0mscaling_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscaling_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscaling_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "X, Y = X[:10000, :], Y[:10000]\n",
    "import warnings\n",
    "warnings.simplefilter(\"error\")\n",
    "w_history, b_history = learn_private_regression(1, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfcol=c(2,1))\n",
    "\n",
    "all.ylim<-c( min(c(history[,1],output$coef[1] )), max(c(history[,1],output$coef[1] )))\n",
    "plot(history[,1], type=\"l\", ylim=all.ylim, ylab=\"beta 0\", xlab=\"step\", lwd=1.5)\n",
    "abline(h=output$coef[1], lty=2, col=\"blue\", lwd=1.5)\n",
    "\n",
    "\n",
    "all.ylim<-c( min(c(history[,2],output$coef[2] )), max(c(history[,2],output$coef[2] )))\n",
    "plot(history[,2], type=\"l\", ylim=all.ylim, ylab=\"beta 1\", xlab=\"step\", lwd=1.5)\n",
    "abline(h=output$coef[2], lty=2, col=\"blue\", lwd=1.5)\n",
    "\n",
    "dev.copy2pdf(file=\"./figs/dpSGD.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
