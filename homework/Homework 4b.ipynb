{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4b Submission\n",
    "## Yonadav Shavit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from scipy.special import factorial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Let's define some latex commands)\n",
    " \n",
    "$\\newcommand{\\floor}[1]{\\left \\lfloor #1 \\right \\rfloor}$\n",
    "$\\newcommand{\\clamp}[1]{\\left [ #1 \\right ]}$\n",
    "$\\newcommand{\\lap}[1]{Lap\\left ( #1 \\right )}$\n",
    "$\\newcommand{\\lapcdf}[2]{LapCDF\\left ( #1, [-\\inf, #2] \\right )}$\n",
    "$\\newcommand{\\lappdf}[2]{LapPDF\\left ( #1, |_{#2} \\right )}$\n",
    "$\\newcommand{\\lapcdffull}[2]{\\frac{1}{2} + \\frac{1}{2}sgn\\left({#2}\\right)\\left ( 1 - \\exp{\\left ( - \\frac{|#2|}{#1} \\right ) }\\right )}$\n",
    "$\\newcommand{\\lappdffull}[2]{\\frac{1}{2 #1} \\exp{\\left ( - \\frac{#2}{#1}\\right ) }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "### (a)\n",
    "#### i.\n",
    "$GS_f = \\infty$ because the sensitivity is unbounded and so a single point can arbitrarily change the mean.\n",
    "#### ii.\n",
    "$\\min _{x \\in \\mathcal{S}} \\operatorname{LS}_{f}(x) = \\infty$ for the same reasons as above - regardless of any of the other points (i.e. $x$ s.t. we get minimum local sensitivity), a single additional point can arbitrarily change the resulting mean. (For example, to increase the mean by $k$, the new point should be $\\mu_{old} + nk$.)\n",
    "#### iii.\n",
    "$\\mathrm{RS}_{f}^{\\mathcal{H}} = \\frac{b-a}{n}$ because a single point can change only by $b-a$ while remaining in $H$, and so the average can only change by that divided by $n$.\n",
    "#### iv.\n",
    "We can construct an explicit Lipschitz extension by simply clipping each point to $[a, b]$ and then taking the mean and adding Laplace noise with sensitivity $\\frac{b-a}{n\\epsilon}$. This will be increasingly biased as the original data is farther from $[a, b]$, but the sensitivity will remain the same, and on $H$ this agrees with $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "#### i.\n",
    "$GS_f = \\infty$ because, for any large $N$, we can construct a dataset with $\\frac{n}{2}$ data points at $0$, and $\\frac{n}{2}$ datapoints at $N$, and then shifting a single point from $0$ to $N$ shifts the median by $N$, for any arbitrary $N$.\n",
    "#### ii.\n",
    "$\\min _{x \\in \\mathcal{S}} \\operatorname{LS}_{f}(x) = 0$ because if we create a dataset with $3$ points at $0$, no matter how much we change any single point, the other two will still be $0$ and so the median will be unchanged.\n",
    "#### iii.\n",
    "$\\mathrm{RS}_{f}^{\\mathcal{H}} = b-a$ because every point must fall in $[a, b]$, and so the largest possible change in median would be from $a$ to $b$. (This example is possible if half the points minus 1 are at $a$, and the other half are at $b$, and then we move one from $b$ to $a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "#### i.\n",
    "$GS_f = n-1$ because if every node was previously disconnected (meaning the value was $n-1$) we can connect a new node to every old node, meaning that now $0$ nodes are disconnected.\n",
    "#### ii.\n",
    "$\\min _{x \\in \\mathcal{S}} \\operatorname{LS}_{f}(x) = 1$ because if every node in the graph is already connected, the addition of a new node cannot disconnect any existing nodes. The most it can do is add a node with no edges. (Note that our definition of node sensitivity in this case is a little weird, because if we're talking about deleting nodes rather than adding, the minimum local sensitivity is actually $0$, if all edges exist between all nodes.)\n",
    "#### iii.\n",
    "$\\mathrm{RS}_{f}^{\\mathcal{H}} = d$ because the greatest change from adding a single node would be that all existing nodes are unconnected, and then a single new node is added that connects to $d$ other nodes. \n",
    "(It can't connect to more than that because it must be in $H$). \n",
    "Thus the query can only decrease by $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the R code, reproduced. I'm porting it into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the likelihood function for a Logit\n",
    "# calcllik<-function(b,data){           \n",
    "#   y<-data[,1]\n",
    "#   x<-data[,2]\n",
    "\n",
    "#   pi<- 1/(1+exp(-b[1] - b[2]*x))        # Here is the systematic component\n",
    "\n",
    "#   llik<-y * log(pi) + (1-y) * log(1-pi) # Here is the stocastic component\n",
    "#   return(-llik)\n",
    "# }\n",
    "\n",
    "def likelihood(w, b, x, y):\n",
    "    pi = 1/(1 + np.exp(-b - w*x.reshape(-1)))\n",
    "    llik = y*np.log(pi + 1e-9) + (1-y)*np.log(1e-9 + 1-pi)\n",
    "    return - llik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Differentially private mean release\n",
    "\n",
    "# gaussianReleaseNoise <- function(size=1, sensitivity, epsilon, delta){\n",
    "# \tscale <- sensitivity *log(1.25/delta)/ epsilon\n",
    "# \tnoise <- rnorm(n=size, mean=0, sd=scale)\n",
    "# \treturn(noise)\n",
    "# }\n",
    "\n",
    "def gaussianReleaseNoise(sensitivity, epsilon, delta, size=1):\n",
    "    scale = sensitivity*np.log(1.25/delta)/epsilon\n",
    "    noise = np.random.normal(scale=scale, size=size)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FutureWarning",
     "evalue": "Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFutureWarning\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-92c9273ec8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"slope = {lr.coef_[0]}, bias = {lr.intercept_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1278\u001b[0m                              \"positive; got (tol=%r)\" % self.tol)\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_check_solver\u001b[0;34m(solver, penalty, dual)\u001b[0m\n\u001b[1;32m    431\u001b[0m         warnings.warn(\"Default solver will be changed to 'lbfgs' in 0.22. \"\n\u001b[1;32m    432\u001b[0m                       \u001b[0;34m\"Specify a solver to silence this warning.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                       FutureWarning)\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mall_solvers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saga'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFutureWarning\u001b[0m: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/yonadavshavit/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m(433)\u001b[0;36m_check_solver\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    431 \u001b[0;31m        warnings.warn(\"Default solver will be changed to 'lbfgs' in 0.22. \"\n",
      "\u001b[0m\u001b[0;32m    432 \u001b[0;31m                      \u001b[0;34m\"Specify a solver to silence this warning.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 433 \u001b[0;31m                      FutureWarning)\n",
      "\u001b[0m\u001b[0;32m    434 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    435 \u001b[0;31m    \u001b[0mall_solvers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saga'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n"
     ]
    }
   ],
   "source": [
    "#### Run with actual data\n",
    "\n",
    "# library(\"foreign\")\n",
    "# PUMSdata <- read.csv(file=\"../../data/MaPUMS5full.csv\")\n",
    "\n",
    "# mydata<-PUMSdata[c(\"married\",\"educ\")]\n",
    "\n",
    "# output <- glm(married ~ educ, family=\"binomial\", data=mydata)\n",
    "\n",
    "# print(summary(output))\n",
    "\n",
    "PUMSdata = pd.read_csv(\"../data/MaPUMS5full.csv\")\n",
    "X = (PUMSdata['educ'].values).reshape(-1, 1)\n",
    "Y = PUMSdata['married'].values\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=10000).fit(X, Y)\n",
    "\n",
    "print(f\"slope = {lr.coef_[0]}, bias = {lr.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Show the estimated model\n",
    "\n",
    "# xseq <- seq(from=-40, to=60, length=100)\n",
    "# f <- 1/(1 + exp(-output$coef[1] -output$coef[2]*xseq))\n",
    "\n",
    "# par(mfcol=c(2,1))\n",
    "\n",
    "# plot(xseq, f, type=\"l\", lwd=1.5, col=\"red\", ylim=c(0,1), ylab=\"E(y|x,theta)\", xlab=\"education\", main=\"Probability Married by Education\")\n",
    "# abline(v=1, col=\"blue\", lty=3)\n",
    "# abline(v=16, col=\"blue\", lty=3)\n",
    "\n",
    "# plot(xseq, f, type=\"l\", lwd=1.5, col=\"red\", ylim=c(0,1), ylab=\"E(y|x,theta)\", xlab=\"education\", xlim=c(-5,20))\n",
    "# for(i in 1:16){\n",
    "# \tflag<-mydata$educ==i\n",
    "# \tpoints(x=i, y=mean(mydata[flag,\"married\"]))\n",
    "# }\n",
    "# dev.copy2pdf(file=\"./figs/married_educ.pdf\")\n",
    "\n",
    "\n",
    "# #### Show the LogLikelihood surface\n",
    "\n",
    "# sample.data <- mydata[sample(1:nrow(mydata),10000), ]\n",
    "# b1.seq <- seq(from=-3, to=2, length=25)\n",
    "# b2.seq <- seq(from=-.2, to=.3, length=25)\n",
    "# llsurface <- matrix(NA, nrow=length(b1.seq), ncol=length(b2.seq))\n",
    "\n",
    "# for(i in 1:length(b1.seq)){\n",
    "# \tfor(j in 1:length(b2.seq)){\n",
    "# \t\tllsurface[i,j] <- sum(-1* calcllik(b=c(b1.seq[i], b2.seq[j]), data=sample.data) )\n",
    "# \t}\n",
    "# }\n",
    "\n",
    "# filled.contour(x=b1.seq, y=b2.seq, z=llsurface, color = terrain.colors,  xlab=\"constant parameter\", ylab=\"education parameter\")\n",
    "\n",
    "# dev.copy2pdf(file=\"./figs/logitLLike.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcgradient <- function(B, C, theta, fun){\n",
    "# \tdx <- 0.0001\n",
    "\n",
    "# \tout1 <-\teval(fun(b=theta, data=B))\n",
    "# \tout2 <- eval(fun(b=theta + c(0,dx), data=B))\n",
    "# \tout3 <- eval(fun(b=theta + c(dx,0), data=B))\n",
    "\n",
    "# \tDel.1 <- 1\n",
    "# \t# Del.1 <- clip(Del.1, lower=0, upper=1)  # Fix this\n",
    "# \tmean.Del.1 <- mean(Del.1)\n",
    "\n",
    "# \tDel.2 <- 1\n",
    "# \t# Del.2 <- clip(Del.2, lower=0, upper=1)  # Fix this\n",
    "# \tmean.Del.2 <- mean(Del.2)\n",
    "\n",
    "# \treturn(c(mean.Del.1,mean.Del.2))\n",
    "# }\n",
    "\n",
    "def calc_clipped_gradient(X, Y, C, w, b, fun=likelihood):\n",
    "    dx = 1e-4\n",
    "    out1 = fun(w     ,      b, X, Y)\n",
    "    out2 = fun(w     , b + dx, X, Y)\n",
    "    out3 = fun(w + dx, b     , X, Y)\n",
    "    dw = (out3 - out1)/dx\n",
    "    db = (out2 - out1)/dx\n",
    "    scaling_param = np.minimum(1, C/np.sqrt(dw**2 + db**2 + 1e-10))\n",
    "    return dw*scaling_param, db*scaling_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N <- nrow(mydata)\n",
    "# L <- round(sqrt(nrow(mydata)))\n",
    "\n",
    "# steps <- 10   \t  # Fix this\n",
    "\n",
    "# ## Shuffle the data\n",
    "# index <- sample(1:nrow(mydata))\n",
    "# mydata <- mydata[index,]\n",
    "# epsilon <-1\n",
    "\n",
    "# theta <- c(0,0)   # Starting parameters\n",
    "# C <- 10\t\t\t  # Interval to clip over\n",
    "# nu <- c(1,0.01)   # Learning speeds\n",
    "\n",
    "\n",
    "# history <- matrix(NA, nrow=steps+1, ncol=2)\n",
    "# history[1,] <- theta\n",
    "\n",
    "# for(i in 1:steps){\n",
    "# \tstartB <- ((i-1)*L+1)\n",
    "# \tif(i<L){\n",
    "# \t\tstopB <- i*L\n",
    "# \t}else{\n",
    "# \t\tstopB <- nrow(mydata)\n",
    "# \t}\n",
    "\n",
    "# \tindex<-sample(1:nrow(mydata),L)\n",
    "# \tB <- mydata[startB:stopB, ]\n",
    "# \tDel <- calcgradient(B, C, theta, fun=calcllik)\n",
    "# \tcat(\"Del:  \",Del,\"\\n\")\n",
    "# \ttheta <- theta   \t\t\t\t# Fix this\n",
    "# \tcat(\"Theta:\",theta, \"\\n\")\n",
    "\n",
    "# \thistory[i+1,] <- theta\n",
    "\n",
    "# }\n",
    "def learn_private_regression(epsilon, X_, Y_, delta=1e-6):\n",
    "    n = Y_.shape[0]\n",
    "    batchsize = int(np.sqrt(n))\n",
    "    steps = int(np.sqrt(n))\n",
    "\n",
    "    idxs = np.random.permutation(n)\n",
    "    X = X_[idxs, :]\n",
    "    Y = Y_[idxs]\n",
    "\n",
    "    w = 0; b = 0         # starting parameters\n",
    "    C = 10               # clipping parameter\n",
    "    nu_w, nu_b = .01, 1  # learning rates\n",
    "\n",
    "    w_history = [w]\n",
    "    b_history = [b]\n",
    "    \n",
    "    for i in tqdm_notebook(range(steps)):\n",
    "        print(f\"w: {w}, b:{b}\")\n",
    "        batch_start = i*batchsize\n",
    "        batch_end = min((i + 1)*batchsize, n)\n",
    "        Xbatch = X[batch_start:batch_end, :]\n",
    "        Ybatch = Y[batch_start:batch_end]\n",
    "        dw_clipped, db_clipped = calc_clipped_gradient(Xbatch, Ybatch, C, w, b, fun=likelihood)\n",
    "        dw_private = dw_clipped + gaussianReleaseNoise(\n",
    "            sensitivity=2*C, # one sample can shift from -C to C, so double sensitivity\n",
    "            epsilon=epsilon/(batchsize*steps*2), # two parameters released, so halve the epsilon for each\n",
    "            delta=delta,\n",
    "            size=batchsize,\n",
    "        )\n",
    "        db_private = db_clipped  + gaussianReleaseNoise(\n",
    "            sensitivity=2*C/batchsize, # one sample can shift from -C to C, so double sensitivity\n",
    "            epsilon=(epsilon/(2*steps)), # two parameters released, so halve the epsilon for each\n",
    "            delta=delta,\n",
    "            size=batchsize,\n",
    "        )\n",
    "        dw_private = dw_private.mean()\n",
    "        db_private = db_private.mean()\n",
    "        # update via calculated gradients\n",
    "        w += nu_w*dw_private\n",
    "        b += nu_b*db_private\n",
    "        w_history.append(w)\n",
    "        b_history.append(b)\n",
    "        if True:\n",
    "            print(f\"Likelihood: {likelihood(w, b, X, Y).mean()}\")\n",
    "    return w_history, b_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d1071d1ad448dc935f54b0bc32cbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 0, b:0\n",
      "Likelihood: 8.519334550965205\n",
      "w: 1791.160662555309, b:50.68680664625842\n",
      "Likelihood: 12.203931250966642\n",
      "w: -2793.0142632563116, b:117.68742042564939\n",
      "Likelihood: 8.519334550965205\n",
      "w: 2287.8427756655633, b:103.23566235159365\n",
      "Likelihood: 8.519334550965205\n",
      "w: 7266.856880820313, b:66.78620676939012\n",
      "Likelihood: 8.519334550965205\n",
      "w: 2598.673508433335, b:-92.63440911105995\n",
      "Likelihood: 12.203931250966642\n",
      "w: -1456.5704188596123, b:-134.1724214997215\n",
      "Likelihood: 12.203931250966642\n",
      "w: -3649.819747948875, b:-198.9463118062282\n",
      "Likelihood: 12.203931250966642\n",
      "w: -5966.937404656933, b:-248.94832127377646\n",
      "Likelihood: 12.203931250966642\n",
      "w: -1155.4808774957646, b:-312.60849417978005\n",
      "Likelihood: 12.203931250966642\n",
      "w: -7946.4149656418485, b:-386.3319932387509\n",
      "Likelihood: 8.519334550965205\n",
      "w: 2346.0385134114276, b:-356.6531242268727\n",
      "Likelihood: 8.519334550965205\n",
      "w: 5142.1644350813585, b:-425.3175189239664\n",
      "Likelihood: 12.203931250966642\n",
      "w: -430.2149104858463, b:-348.25850869197455\n",
      "Likelihood: 12.203931250966642\n",
      "w: -4919.978043477307, b:-359.9614678352605\n",
      "Likelihood: 12.203931250966642\n",
      "w: -6562.7264322533765, b:-399.18710044872654\n",
      "Likelihood: 12.203931250966642\n",
      "w: -14124.080085740748, b:-498.6505090279431\n",
      "Likelihood: 12.203931250966642\n",
      "w: -12900.754869830253, b:-504.79548824233376\n",
      "Likelihood: 12.203931250966642\n",
      "w: -7898.368613101691, b:-470.7882154847848\n",
      "Likelihood: 12.203931250966642\n",
      "w: -5041.129809346745, b:-454.4922059028171\n",
      "Likelihood: 12.203931250966642\n",
      "w: -4514.869248438608, b:-615.1245666818998\n",
      "Likelihood: 12.203931250966642\n",
      "w: -14608.079994778253, b:-661.5522423065936\n",
      "Likelihood: 12.203931250966642\n",
      "w: -21524.333711873143, b:-729.9364194706727\n",
      "Likelihood: 12.203931250966642\n",
      "w: -20853.77472967796, b:-865.3090614065623\n",
      "Likelihood: 12.203931250966642\n",
      "w: -17632.8704335658, b:-871.4972321072303\n",
      "Likelihood: 12.203931250966642\n",
      "w: -21722.070401742592, b:-879.9178132419996\n",
      "Likelihood: 12.203931250966642\n",
      "w: -21944.83447528177, b:-816.4765347392884\n",
      "Likelihood: 12.203931250966642\n",
      "w: -19678.03831336637, b:-825.9052875032328\n",
      "Likelihood: 12.203931250966642\n",
      "w: -16297.258093271479, b:-861.4192462830715\n",
      "Likelihood: 12.203931250966642\n",
      "w: -19697.52299948052, b:-861.0283627036008\n",
      "Likelihood: 12.203931250966642\n",
      "w: -26244.016747258444, b:-881.5903520079978\n",
      "Likelihood: 12.203931250966642\n",
      "w: -29824.497985392394, b:-858.77108779559\n",
      "Likelihood: 12.203931250966642\n",
      "w: -29985.646275514388, b:-919.4984937555621\n",
      "Likelihood: 12.203931250966642\n",
      "w: -32456.772072033273, b:-822.4812159155019\n",
      "Likelihood: 12.203931250966642\n",
      "w: -29707.69760134133, b:-781.4518767166131\n",
      "Likelihood: 12.203931250966642\n",
      "w: -31561.01288173296, b:-750.9634130521392\n",
      "Likelihood: 12.203931250966642\n",
      "w: -31136.33703148797, b:-772.2814445072397\n",
      "Likelihood: 12.203931250966642\n",
      "w: -32317.48975180656, b:-825.8228087450583\n",
      "Likelihood: 12.203931250966642\n",
      "w: -31596.528019571448, b:-815.8939599475794\n",
      "Likelihood: 12.203931250966642\n",
      "w: -28101.520975180836, b:-841.2100862726666\n",
      "Likelihood: 12.203931250966642\n",
      "w: -28825.76251175165, b:-849.6969504163729\n",
      "Likelihood: 12.203931250966642\n",
      "w: -26695.628887486124, b:-886.0850641560353\n",
      "Likelihood: 12.203931250966642\n",
      "w: -29519.631732690046, b:-849.0882644632854\n",
      "Likelihood: 12.203931250966642\n",
      "w: -24948.925949473003, b:-832.5469684270674\n",
      "Likelihood: 12.203931250966642\n",
      "w: -29753.404772546233, b:-762.4486404980456\n",
      "Likelihood: 12.203931250966642\n",
      "w: -29728.69197555563, b:-719.729951039592\n",
      "Likelihood: 12.203931250966642\n",
      "w: -28652.50895656521, b:-760.5908412454103\n",
      "Likelihood: 12.203931250966642\n",
      "w: -35333.00868411796, b:-765.102677670552\n",
      "Likelihood: 12.203931250966642\n",
      "w: -42475.322239682515, b:-753.3855150309583\n",
      "Likelihood: 12.203931250966642\n",
      "w: -39071.456866789296, b:-768.2287812777116\n",
      "Likelihood: 12.203931250966642\n",
      "w: -40878.098437713976, b:-728.3933284279963\n",
      "Likelihood: 12.203931250966642\n",
      "w: -41270.29014143066, b:-741.4342404250217\n",
      "Likelihood: 12.203931250966642\n",
      "w: -47376.05427455183, b:-696.7121541109474\n",
      "Likelihood: 12.203931250966642\n",
      "w: -53070.71865489851, b:-762.9120126613115\n",
      "Likelihood: 12.203931250966642\n",
      "w: -56546.659396104944, b:-873.7580165954164\n",
      "Likelihood: 12.203931250966642\n",
      "w: -59318.92341362234, b:-808.3746844440027\n",
      "Likelihood: 12.203931250966642\n",
      "w: -59855.520237159784, b:-732.9055618435648\n",
      "Likelihood: 12.203931250966642\n",
      "w: -60890.85346454872, b:-727.97961163012\n",
      "Likelihood: 12.203931250966642\n",
      "w: -54767.97583709769, b:-679.3858181983592\n",
      "Likelihood: 12.203931250966642\n",
      "w: -54826.03603249277, b:-611.655686069458\n",
      "Likelihood: 12.203931250966642\n",
      "w: -59422.14296886431, b:-656.7220064286628\n",
      "Likelihood: 12.203931250966642\n",
      "w: -58483.69716039164, b:-683.3851810559254\n",
      "Likelihood: 12.203931250966642\n",
      "w: -64858.97309504639, b:-631.6693905567429\n",
      "Likelihood: 12.203931250966642\n",
      "w: -70117.45826739815, b:-785.4146726147536\n",
      "Likelihood: 12.203931250966642\n",
      "w: -69075.04856615991, b:-864.0218200720672\n",
      "Likelihood: 12.203931250966642\n",
      "w: -80260.18045080008, b:-879.5073166899466\n",
      "Likelihood: 12.203931250966642\n",
      "w: -89418.47766803656, b:-910.3388495999731\n",
      "Likelihood: 12.203931250966642\n",
      "w: -80378.00942615246, b:-837.3510542993869\n",
      "Likelihood: 12.203931250966642\n",
      "w: -78675.98824179897, b:-795.6690980393809\n",
      "Likelihood: 12.203931250966642\n",
      "w: -90451.10854650759, b:-827.8068676980572\n",
      "Likelihood: 12.203931250966642\n",
      "w: -97649.3432894126, b:-911.9015137698899\n",
      "Likelihood: 12.203931250966642\n",
      "w: -89632.83643892371, b:-890.6398499741626\n",
      "Likelihood: 12.203931250966642\n",
      "w: -87437.40203083346, b:-855.6556444219341\n",
      "Likelihood: 12.203931250966642\n",
      "w: -89906.5473695886, b:-863.8432971473782\n",
      "Likelihood: 12.203931250966642\n",
      "w: -94413.8127763687, b:-877.2905375009927\n",
      "Likelihood: 12.203931250966642\n",
      "w: -99078.2060082888, b:-821.5742842724734\n",
      "Likelihood: 12.203931250966642\n",
      "w: -95820.34955758657, b:-865.7505565322508\n",
      "Likelihood: 12.203931250966642\n",
      "w: -93419.14134184978, b:-914.4276616590383\n",
      "Likelihood: 12.203931250966642\n",
      "w: -86425.68609225134, b:-914.8613326270832\n",
      "Likelihood: 12.203931250966642\n",
      "w: -94843.5434253583, b:-889.0317204154576\n",
      "Likelihood: 12.203931250966642\n",
      "w: -96789.71246927136, b:-790.4765102606841\n",
      "Likelihood: 12.203931250966642\n",
      "w: -94246.6424880619, b:-718.7286156663782\n",
      "Likelihood: 12.203931250966642\n",
      "w: -85368.35850399564, b:-650.5256217972066\n",
      "Likelihood: 12.203931250966642\n",
      "w: -80274.353816292, b:-660.3950760140598\n",
      "Likelihood: 12.203931250966642\n",
      "w: -80586.01385441991, b:-669.3164441813858\n",
      "Likelihood: 12.203931250966642\n",
      "w: -90373.54358717072, b:-699.7302475114822\n",
      "Likelihood: 12.203931250966642\n",
      "w: -83758.65214201281, b:-750.031181830558\n",
      "Likelihood: 12.203931250966642\n",
      "w: -87698.67550156638, b:-810.1423265696991\n",
      "Likelihood: 12.203931250966642\n",
      "w: -86569.1984288851, b:-796.7869681973799\n",
      "Likelihood: 12.203931250966642\n",
      "w: -78381.5755732473, b:-765.5310656878142\n",
      "Likelihood: 12.203931250966642\n",
      "w: -81668.6367744694, b:-692.4848287635995\n",
      "Likelihood: 12.203931250966642\n",
      "w: -84050.74030172669, b:-714.9158404287857\n",
      "Likelihood: 12.203931250966642\n",
      "w: -80337.78060430098, b:-653.9760598631873\n",
      "Likelihood: 12.203931250966642\n",
      "w: -89108.4711381162, b:-604.5728507704723\n",
      "Likelihood: 12.203931250966642\n",
      "w: -92457.88097235578, b:-590.1137793426985\n",
      "Likelihood: 12.203931250966642\n",
      "w: -85446.47106838915, b:-590.2850156561011\n",
      "Likelihood: 12.203931250966642\n",
      "w: -89831.93762366289, b:-623.0355015334525\n",
      "Likelihood: 12.203931250966642\n",
      "w: -88431.79803291889, b:-600.1457822540735\n",
      "Likelihood: 12.203931250966642\n",
      "w: -90707.61083147113, b:-513.669092643828\n",
      "Likelihood: 12.203931250966642\n",
      "w: -93108.37014323774, b:-502.4140446877924\n",
      "Likelihood: 12.203931250966642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = X[:10000, :], Y[:10000]\n",
    "import warnings\n",
    "warnings.simplefilter(\"error\")\n",
    "w_history, b_history = learn_private_regression(1, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfcol=c(2,1))\n",
    "\n",
    "all.ylim<-c( min(c(history[,1],output$coef[1] )), max(c(history[,1],output$coef[1] )))\n",
    "plot(history[,1], type=\"l\", ylim=all.ylim, ylab=\"beta 0\", xlab=\"step\", lwd=1.5)\n",
    "abline(h=output$coef[1], lty=2, col=\"blue\", lwd=1.5)\n",
    "\n",
    "\n",
    "all.ylim<-c( min(c(history[,2],output$coef[2] )), max(c(history[,2],output$coef[2] )))\n",
    "plot(history[,2], type=\"l\", ylim=all.ylim, ylab=\"beta 1\", xlab=\"step\", lwd=1.5)\n",
    "abline(h=output$coef[2], lty=2, col=\"blue\", lwd=1.5)\n",
    "\n",
    "dev.copy2pdf(file=\"./figs/dpSGD.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
